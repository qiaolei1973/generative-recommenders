# 🎓 Generative Recommenders 核心算法详解

> 为入门开发者准备的完整学习指南

## 📚 目录

1. [核心思想：从传统推荐到生成式推荐](#1-核心思想)
2. [关键算法详解](#2-关键算法详解)
3. [数据结构创新](#3-数据结构创新)
4. [训练策略](#4-训练策略)
5. [性能优化技术](#5-性能优化技术)
6. [实践案例](#6-实践案例)

---

## 1. 核心思想：从传统推荐到生成式推荐

### 🔄 范式转变

#### 传统推荐系统（DLRM）
```
用户特征 + 物品特征 → 打分模型 → 预测评分
```
**问题**：
- 需要为每个物品计算分数（计算量大）
- 难以捕捉序列关系
- 扩展性差

#### 生成式推荐系统（GR）
```
用户历史序列 → 生成模型 → 直接生成下一个物品
```
**优势**：
- 像语言模型一样生成推荐
- 自然建模序列关系
- 可扩展到万亿参数

### 📖 类比理解

想象你在看电影：

**传统方法**：
```
系统：这里有 10000 部电影，我给每部打个分
你：太慢了！
```

**生成式方法**：
```
系统：你看了《玩具总动员》→《阿甘正传》→《肖申克的救赎》
系统：下一部推荐《辛德勒的名单》（直接生成）
你：快多了！
```

---

## 2. 关键算法详解

### 🌟 算法 1: HSTU (Hierarchical Sequential Transducer Unit)

这是项目的**核心算法**，发表在 ICML 2024。

#### 2.1 什么是 HSTU？

HSTU 是一个**分层序列转换单元**，专门为推荐系统设计的 Transformer 变体。

**核心创新**：
1. **层次化设计**：多层 STU 堆叠
2. **高效注意力**：优化的自注意力机制
3. **序列建模**：专门处理用户行为序列

#### 2.2 HSTU 架构图解

```
用户历史序列
    ↓
[电影1, 电影2, ..., 电影N]
    ↓
┌─────────────────────────────┐
│  Embedding Layer            │  ← 将电影ID转为向量
│  [向量1, 向量2, ..., 向量N] │
└─────────────────────────────┘
    ↓
┌─────────────────────────────┐
│  STU Layer 1                │  ← 第一层转换
│  ├─ LayerNorm               │
│  ├─ Multi-Head Attention    │  ← 学习电影之间的关系
│  ├─ Feed Forward Network    │
│  └─ Residual Connection     │
└─────────────────────────────┘
    ↓
┌─────────────────────────────┐
│  STU Layer 2                │  ← 第二层转换
│  ├─ LayerNorm               │
│  ├─ Multi-Head Attention    │
│  ├─ Feed Forward Network    │
│  └─ Residual Connection     │
└─────────────────────────────┘
    ↓
┌─────────────────────────────┐
│  Output Layer               │  ← 预测下一部电影
│  预测: 电影X (概率: 0.85)   │
└─────────────────────────────┘
```

#### 2.3 STU Layer 详解

每个 STU Layer 包含以下组件：

##### A. Layer Normalization（层归一化）
```python
# 作用：稳定训练，加速收敛
normalized_x = (x - mean) / sqrt(variance + epsilon)
```

**为什么需要**：
- 防止梯度爆炸/消失
- 让不同层的输入分布稳定

##### B. Multi-Head Attention（多头注意力）
```python
# 核心思想：让模型关注序列中的重要信息

# 1. 计算 Query, Key, Value
Q = x @ W_q  # 查询：我在找什么？
K = x @ W_k  # 键：我有什么信息？
V = x @ W_v  # 值：具体的信息内容

# 2. 计算注意力分数
scores = Q @ K^T / sqrt(d_k)  # 计算相似度
attention_weights = softmax(scores)  # 归一化为概率

# 3. 加权求和
output = attention_weights @ V  # 根据重要性加权
```

**实际例子**：
```
用户看了：[玩具总动员, 阿甘正传, 肖申克的救赎]

注意力机制学到：
- "玩具总动员" 和 "肖申克的救赎" 关系不大 (权重: 0.1)
- "阿甘正传" 和 "肖申克的救赎" 都是经典剧情片 (权重: 0.8)
→ 预测下一部可能是类似的经典剧情片
```

##### C. Feed Forward Network（前馈网络）
```python
# 两层全连接网络
FFN(x) = max(0, x @ W1 + b1) @ W2 + b2
       = ReLU(Linear(x)) @ Linear
```

**作用**：
- 增加模型的非线性表达能力
- 对每个位置独立处理

##### D. Residual Connection（残差连接）
```python
output = x + Attention(LayerNorm(x))
output = output + FFN(LayerNorm(output))
```

**为什么需要**：
- 解决深层网络训练困难
- 让梯度更容易反向传播

#### 2.4 HSTU 的特殊优化

##### 🎯 Causal Attention（因果注意力）
```
在推荐系统中，只能看"过去"，不能看"未来"

时间线：[电影1] → [电影2] → [电影3] → [?]

注意力掩码：
     电影1  电影2  电影3
电影1  ✓     ✗     ✗      只能看自己
电影2  ✓     ✓     ✗      能看1和2
电影3  ✓     ✓     ✓      能看1,2,3
```

##### 🎯 Relative Position Bias（相对位置偏置）
```python
# 考虑电影之间的时间距离
bias = f(position_i - position_j, timestamp_i - timestamp_j)

# 例子：
# 昨天看的电影 vs 一年前看的电影
# → 昨天的更重要（权重更高）
```

---

### 🌟 算法 2: SASRec (Self-Attentive Sequential Recommendation)

这是**对比基准算法**，也是 HSTU 的前身。

#### 2.1 SASRec 架构

```
用户序列
    ↓
Embedding
    ↓
Self-Attention Block × N  ← 标准 Transformer
    ↓
Prediction Layer
```

#### 2.2 HSTU vs SASRec

| 特性 | SASRec | HSTU |
|------|--------|------|
| 注意力机制 | 标准 Self-Attention | 优化的 HSTU Attention |
| 前馈网络 | 标准 FFN | UVQK 分解 |
| 位置编码 | 学习的位置嵌入 | 相对位置偏置 |
| 计算效率 | 基准 | **10x-1000x 加速** |
| 性能 | 基准 | **+8.6% HR@10** |

---

### 🌟 算法 3: Sampled Softmax Loss（采样 Softmax 损失）

#### 3.1 为什么需要采样？

**问题**：
```
物品数量：3,706 部电影
传统 Softmax：需要计算 3,706 个分数
→ 太慢了！
```

**解决方案**：
```
采样 Softmax：只计算 128 个分数
- 1 个正样本（用户真正看的）
- 127 个负样本（随机采样的）
→ 快 29 倍！
```

#### 3.2 采样策略

##### A. In-Batch Sampling（批内采样）
```python
# 利用同一批次中其他用户的正样本作为负样本

Batch:
用户1 → 电影A (正样本)
用户2 → 电影B (正样本)
用户3 → 电影C (正样本)

对于用户1：
- 正样本：电影A
- 负样本：电影B, 电影C（来自其他用户）
```

**优点**：
- 不需要额外采样
- 计算高效

##### B. Local Sampling（局部采样）
```python
# 从整个物品集合中随机采样

对于用户1：
- 正样本：电影A
- 负样本：从 3,706 部电影中随机选 127 部
```

**优点**：
- 更多样化的负样本
- 更好的泛化能力

#### 3.3 温度缩放（Temperature Scaling）

```python
# 控制预测的"锐利度"
logits = scores / temperature

temperature = 0.05  # 低温度 → 更确定的预测
temperature = 1.0   # 高温度 → 更平滑的预测
```

**实际效果**：
```
temperature = 0.05:
电影A: 0.85  ← 非常确定
电影B: 0.10
电影C: 0.05

temperature = 1.0:
电影A: 0.50  ← 不太确定
电影B: 0.30
电影C: 0.20
```

---

## 3. 数据结构创新

### 🎯 Jagged Tensors（不规则张量）

#### 3.1 问题：变长序列

```
用户1: [电影1, 电影2, 电影3]           (长度: 3)
用户2: [电影1, 电影2, ..., 电影100]   (长度: 100)
用户3: [电影1, 电影2]                  (长度: 2)
```

#### 3.2 传统方法：Padding（填充）

```python
# 填充到最大长度
用户1: [电影1, 电影2, 电影3, PAD, PAD, ..., PAD]  # 97个PAD
用户2: [电影1, 电影2, ..., 电影100]               # 0个PAD
用户3: [电影1, 电影2, PAD, PAD, ..., PAD]         # 98个PAD

问题：
- 浪费内存（存储大量 PAD）
- 浪费计算（计算 PAD 的注意力）
- 效率低下
```

#### 3.3 Jagged Tensors 方法

```python
# 紧凑存储，不浪费空间
values = [电影1, 电影2, 电影3,  # 用户1
          电影1, 电影2, ..., 电影100,  # 用户2
          电影1, 电影2]  # 用户3

offsets = [0, 3, 103, 105]  # 每个用户的起始位置

优势：
- 零内存浪费
- 零计算浪费
- 10x-100x 加速
```

#### 3.4 Jagged 操作示例

```python
# 连接两个 Jagged Tensors
jagged1 = [A, B, C | D, E]  # 用户1: ABC, 用户2: DE
jagged2 = [X, Y | Z]        # 用户1: XY, 用户2: Z

concat(jagged1, jagged2) = [A, B, C, X, Y | D, E, Z]
                           # 用户1: ABCXY, 用户2: DEZ

# 分割 Jagged Tensor
split(jagged, [3, 2]) = [A, B, C | D, E], [X, Y | Z]
```

---

## 4. 训练策略

### 🎯 训练目标

```
给定：用户历史 [电影1, 电影2, ..., 电影N]
预测：下一个电影 电影(N+1)
```

### 🎯 训练流程

```python
for epoch in range(num_epochs):
    for batch in dataloader:
        # 1. 获取数据
        user_sequences = batch['sequence_item_ids']  # [B, L]
        targets = batch['targets']  # [B]
        
        # 2. 前向传播
        user_embeddings = model(user_sequences)  # [B, D]
        
        # 3. 采样负样本
        negative_items = sample_negatives(num_negatives=128)
        
        # 4. 计算相似度
        scores = user_embeddings @ item_embeddings.T  # [B, 128]
        
        # 5. 计算损失
        loss = sampled_softmax_loss(scores, targets)
        
        # 6. 反向传播
        loss.backward()
        optimizer.step()
```

### 🎯 评估指标

#### HR@K (Hit Rate at K)
```
前K个推荐中是否包含真实物品

例子：
真实：电影A
推荐Top10：[电影B, 电影A, 电影C, ...]
HR@10 = 1 (命中！)

推荐Top10：[电影B, 电影C, 电影D, ...]
HR@10 = 0 (未命中)
```

#### NDCG@K (Normalized Discounted Cumulative Gain)
```
考虑排名位置的质量指标

真实：电影A
推荐：[电影A, 电影B, ...]  → NDCG = 1.0 (最好)
推荐：[电影B, 电影A, ...]  → NDCG = 0.63 (还行)
推荐：[..., ..., 电影A]    → NDCG = 0.3 (不好)
```

---

## 5. 性能优化技术

### 🚀 优化 1: FlashAttention

```
标准注意力：O(N²) 内存
FlashAttention：O(N) 内存

加速：2-4x
```

### 🚀 优化 2: Triton 内核

```python
# 自定义 GPU 内核，针对推荐系统优化
@triton.jit
def jagged_dense_bmm_kernel(...):
    # 高效的 Jagged Tensor 矩阵乘法
    # 10x-100x 加速
```

### 🚀 优化 3: 重计算（Recomputation）

```python
# 训练时：不保存中间激活，需要时重新计算
# 优势：节省显存，可以训练更大的模型
# 代价：增加少量计算时间

recompute_normed_x = True      # 重计算归一化
recompute_uvqk = True          # 重计算注意力中间值
activation_checkpoint = True   # 激活检查点
```

### 🚀 优化 4: 混合精度训练

```python
# BF16 (Brain Float 16)：16位浮点数
# 优势：
# - 减少一半显存
# - 加速 2x
# - 保持数值稳定性

with torch.autocast('cuda', dtype=torch.bfloat16):
    output = model(input)
```

---

## 6. 实践案例

### 📝 案例 1: 预测下一部电影

```python
# 用户历史
user_history = [
    "Toy Story (1995)",           # 动画/喜剧
    "Forrest Gump (1994)",        # 剧情
    "The Shawshank Redemption"    # 剧情
]

# HSTU 模型处理
embeddings = embed(user_history)  # 转为向量
encoded = hstu_model(embeddings)  # 编码
prediction = predict(encoded)     # 预测

# 输出 Top-5 推荐
Top-5:
1. "The Green Mile" (概率: 0.85)        # 类似剧情片
2. "Schindler's List" (概率: 0.78)     # 经典剧情
3. "Good Will Hunting" (概率: 0.72)    # 剧情/励志
4. "The Godfather" (概率: 0.68)        # 经典电影
5. "Pulp Fiction" (概率: 0.65)         # 经典/剧情
```

### 📝 案例 2: 模型训练过程

```
Epoch 1:
  Loss: 2.456
  HR@10: 0.156
  NDCG@10: 0.089

Epoch 20:
  Loss: 1.234
  HR@10: 0.245
  NDCG@10: 0.142

Epoch 50:
  Loss: 0.876
  HR@10: 0.298
  NDCG@10: 0.168

Epoch 100:
  Loss: 0.654
  HR@10: 0.3097  ← HSTU 最终结果
  NDCG@10: 0.1720
```

---

## 🎓 学习路径建议

### 第一阶段：基础概念（1-2周）
1. ✅ 理解推荐系统基本概念
2. ✅ 学习 Transformer 基础（注意力机制）
3. ✅ 了解序列建模问题

**推荐资源**：
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Attention Is All You Need 论文](https://arxiv.org/abs/1706.03762)

### 第二阶段：序列推荐（2-3周）
1. ✅ 学习 SASRec 算法
2. ✅ 理解序列推荐的特点
3. ✅ 实践 MovieLens 数据集

**推荐资源**：
- [SASRec 论文](https://arxiv.org/abs/1808.09781)
- 运行本项目的 SASRec 配置

### 第三阶段：HSTU 深入（3-4周）
1. ✅ 阅读 HSTU 论文
2. ✅ 理解 Jagged Tensors
3. ✅ 学习性能优化技术
4. ✅ 运行和调试代码

**推荐资源**：
- [HSTU 论文](https://arxiv.org/abs/2402.17152)
- 本项目代码和配置文件

### 第四阶段：实践项目（4+周）
1. ✅ 在 MovieLens-1M 上训练模型
2. ✅ 调整超参数
3. ✅ 尝试改进算法
4. ✅ 应用到自己的数据集

---

## 🔑 关键要点总结

### 核心算法
1. **HSTU**: 分层序列转换单元，10x-1000x 加速
2. **Sampled Softmax**: 采样负样本，加速训练
3. **Jagged Tensors**: 高效处理变长序列

### 性能提升
```
SASRec → HSTU:
- HR@10: 0.2853 → 0.3097 (+8.6%)
- NDCG@10: 0.1603 → 0.1720 (+7.3%)
- 训练速度: 10x-1000x 加速
```

### 技术栈
- **模型**: Transformer + 优化
- **数据**: Jagged Tensors
- **训练**: 混合精度 + 重计算
- **推理**: FlashAttention + Triton

---

## 📚 参考资料

1. **HSTU 论文**: [Actions Speak Louder than Words](https://arxiv.org/abs/2402.17152)
2. **SASRec 论文**: [Self-Attentive Sequential Recommendation](https://arxiv.org/abs/1808.09781)
3. **Transformer 论文**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
4. **项目 GitHub**: https://github.com/qiaolei1973/generative-recommenders

---

## 💡 常见问题

### Q1: HSTU 和 Transformer 有什么区别？
A: HSTU 是专门为推荐系统优化的 Transformer 变体，主要改进：
- 使用 Jagged Tensors 处理变长序列
- 优化的注意力机制（UVQK 分解）
- 相对位置偏置而非绝对位置编码
- 针对推荐场景的因果注意力

### Q2: 为什么叫"生成式"推荐？
A: 因为它像语言模型一样，根据历史序列"生成"下一个物品，而不是对所有物品打分排序。

### Q3: Jagged Tensors 有多重要？
A: 非常重要！它是实现 10x-1000x 加速的关键。没有它，处理变长序列会浪费大量计算和内存。

### Q4: 我需要什么基础？
A: 
- Python 编程
- 基础深度学习（PyTorch）
- 线性代数基础
- 了解推荐系统更好（但不是必须）

---

**最后更新**: 2026-01-12
**作者**: AI Assistant
**适用人群**: 推荐算法入门开发者

祝学习愉快！🚀

